# AI and Machine Learning Experience

My experience with AI and machine learning spans several domains, with a practical focus on building systems that work in production rather than just theoretical knowledge. I've worked with LLM integration, reinforcement learning, RAG systems, and traditional machine learning, always prioritizing real-world application over academic exercises.

The most advanced AI work I've done is with large language models, specifically OpenAI's APIs. In my moh-ami French learning tool, I implemented structured prompt engineering to get consistent, parseable JSON outputs from GPT-4o-mini. This wasn't just calling an API - I had to design prompts that request specific JSON schemas, validate the outputs, handle common LLM errors like markdown-wrapped JSON, correct word mapping mistakes, and gracefully handle API failures like quota limits and rate limits. Temperature settings, response format enforcement, and semantic chunking strategies all came from iterative testing and refinement.

My current Folio project is a RAG (Retrieval Augmented Generation) system that combines information retrieval with LLM generation. I'm using LangChain to orchestrate the workflow, Pinecone for vector storage of embeddings, and OpenAI's text-embedding-3-small model to convert text into 1536-dimensional vectors. The system performs semantic similarity search to find relevant context, then passes that context to GPT-4o-mini to generate personalized responses. Understanding embeddings, vector similarity, retrieval strategies, and prompt design for synthesis has been crucial for this project.

In WhatNow, I implemented a reinforcement learning system using contextual bandits. This wasn't using TensorFlow or PyTorch - it's a custom implementation in Python and JavaScript because I needed lightweight code that could run in both backend and frontend environments. The system does feature engineering by combining context attributes (mood, energy, weather) with activity attributes to make personalized recommendations. It uses online learning, meaning the model updates continuously from user feedback rather than training in batches. The two-layer learning architecture (fast Session AI with rate=0.8, slow Base AI with rate=0.02) was my solution to the tradeoff between immediate responsiveness and long-term robustness.

I've attempted traditional machine learning with less success. The Cirrus weather prediction project used XGBoost models for wildfire prediction and implemented a genetic algorithm framework for evolving model parameters through fitness tournaments with crossover and mutation operators. I processed 56 million weather records and 438,000 wildfire events using scikit-learn, NumPy, Pandas, and custom spatial data structures. The project ultimately failed due to fundamental data quality issues, but it taught me crucial lessons about the importance of data quality as a foundation and knowing when to recognize that technical excellence can't overcome inadequate source data.

What I've learned from these experiences is that production AI is very different from academic AI. In production, you deal with API reliability issues, cost constraints (token limits matter when you're paying per request), latency requirements (users won't wait 10 seconds for a response), deployment constraints (models need to be small enough to deploy efficiently), and the reality that datasets are messy and often inadequate. I'm more interested in building AI systems that actually work and provide value than in pursuing the highest possible benchmark scores.

The pattern I've found that works for me is focusing on AI applications where the data acquisition problem is solvable. WhatNow generates its own training data through usage. RAG systems leverage existing knowledge rather than requiring massive training datasets. LLM integration uses pre-trained models that just need good prompt engineering. This pragmatic approach means I can actually finish and deploy projects rather than getting stuck in the data acquisition phase.

I'm excited about the current state of AI, particularly LLMs and embeddings, because they've opened up applications that were previously impossible for individual developers. Being able to integrate powerful AI capabilities into applications without needing massive compute resources or months of training time is transformative, and I'm actively building systems that leverage these tools to create real value.

