# Atlantis: Sensor Integration and Multi-Sensor Fusion

The sensor integration in Atlantis demonstrates sophisticated multi-sensor fusion combining GPS, ultrasonic sensors, accelerometers, and compass data to create a complete environmental awareness system for both the surface boat and underwater probe. This represents the nervous system of the mapping platform.

The original underwater drone sensor suite included four core sensor types working together. The MPU6050 IMU provided 6-axis motion sensing with accelerometer for detecting linear acceleration and orientation, and gyroscope for measuring rotation rates. The BMP280 pressure sensor measured barometric pressure for depth calculation and ambient temperature. The QMC5883L magnetometer gave compass heading for absolute orientation. The HC-SR04 ultrasonic sensor detected obstacles and measured distances. These sensors formed a complete situational awareness package.

Sensor integration required solving I2C bus management challenges. Multiple sensors sharing the I2C communication bus needed proper addressing, and some sensors like the BMP280 had address selection pins requiring careful configuration. I discovered early that using raw I2C register reading led to data corruption and impossible readings - the MPU6050 showed 217°C temperatures and 498°/s rotation rates that were physically impossible. The solution was using Adafruit CircuitPython libraries that handle the low-level I2C communication correctly with proper timing and error handling.

The UART communication architecture separated concerns between processors. The Raspberry Pi 3B runs Python and handles high-level logic like navigation planning, data logging, and GPS processing. The Raspberry Pi Pico runs MicroPython for real-time sensor reading with microsecond precision. UART connects them using GP0/GP1 pins with a custom protocol I developed for sensor data transmission. This achieved sub-25 millisecond response times with 100% success rate after resolving initial I2C corruption issues.

Sensor fusion algorithms combine readings from multiple sensors to produce more reliable results than any single sensor provides. The accelerometer detects the probe's orientation but drifts over time. The gyroscope measures rotation rates accurately but accumulates integration errors. The magnetometer provides absolute heading but is sensitive to magnetic interference. By combining all three with complementary filtering, the system maintains accurate orientation even when individual sensors have momentary issues.

The ultrasonic sensor array in the new probe design uses eight HC-SR04 sensors arranged in a hemispherical pattern. Each sensor points in a different direction, together covering approximately 180 degrees of the hemisphere below the probe. Triggering them in sequence with proper timing avoids crosstalk where one sensor's ultrasonic pulse interferes with another's. The result is eight simultaneous distance measurements creating a detailed point cloud of the area directly below and around the probe.

GPS integration on the surface boat provides absolute positioning with typically 2-5 meter accuracy in open sky conditions. The GPS module communicates via serial interface, sending NMEA sentences containing latitude, longitude, altitude, time, and accuracy metrics. I parse these in Python to extract position data, convert coordinates to the mapping coordinate system, and log positions with each sensor measurement. The stop-and-wait mapping strategy ensures GPS readings are taken when the boat is stationary, maximizing position accuracy.

The cable angle sensor mounted near the probe uses an accelerometer to measure how much the cable deviates from vertical. In perfect conditions with no current, the cable hangs straight down and depth equals cable length. With current or boat motion, the cable angles and actual depth is cable length times cosine of the angle. This sensor enables accurate depth measurement without pressure sensors, which are expensive and require calibration.

Data fusion happens at multiple levels. At the hardware level, individual sensors provide raw measurements. At the firmware level on the Pico, these are filtered and packaged for transmission. At the software level on the Pi 3B, data from multiple sensors is combined with GPS and timing information. At the analysis level in post-processing, measurements from multiple boat positions are triangulated to create the final 3D map with error bounds and confidence scores.

The troubleshooting documentation I created captures recurring issues and proven solutions. Python package installation failures on the Raspberry Pi OS needed the `--break-system-packages` flag. MPU6050 library conflicts between CircuitPython and MicroPython required careful library selection. Power cycling resolved I2C bus corruption. The HC-SR04 needed proper timing delays between trigger pulses. This documentation makes future development faster and helps other developers understand the quirks of hardware integration.

From a portfolio perspective, the sensor integration demonstrates embedded systems programming, I2C and SPI protocols, UART communication, sensor fusion algorithms, real-time data processing, and multi-processor coordination. It shows I can work at the hardware level with actual electronic components, not just web APIs and databases. This breadth beyond typical web development makes me more versatile for roles involving IoT, robotics, or embedded systems.

What I learned from sensor integration is that hardware is messier than software. Code either works or doesn't, but sensors give readings that might be correct, corrupted, noisy, or mis-calibrated. You need defensive programming with sanity checks, redundant measurements, and graceful degradation when sensors fail. The software must work around hardware imperfections rather than assuming perfect inputs.

The sensor suite also taught me about environmental factors. Temperature affects sensor accuracy. Magnetic fields near motors interfere with magnetometers. Water pressure changes sensor behavior. Vibration from motors creates accelerometer noise. Real-world sensor integration means understanding these physical effects and compensating for them in software. This physics awareness makes me a more effective systems engineer.

The evolution from the original underwater drone sensor suite to the current mapping system preserved the core sensor integration learning. Even though the architecture changed completely, the skills of reading I2C sensors, handling UART communication, fusing multi-sensor data, and debugging hardware issues all transferred directly. The eight ultrasonic sensors in the probe use the same HC-SR04 modules and reading techniques I developed for the drone obstacle detection.

What makes the sensor integration particularly valuable is that it's fully working and tested. This isn't theoretical knowledge from tutorials; it's practical experience from building a real system with actual hardware. I've personally held the sensors, connected the wires, written the code, and watched the data flow. When I talk about sensor fusion or I2C protocols, I'm speaking from hands-on experience debugging real problems and creating working solutions.

The comprehensive sensor testing phase in September 2025 validated each sensor individually before integration. Each sensor has test scripts proving it works correctly. This methodical approach of testing components individually before combining them reflects disciplined engineering methodology. It prevented the nightmare of trying to debug a fully integrated system where any component could be the problem.

The Atlantis sensor integration represents the foundation enabling everything else in the project. Without reliable sensor data, GPS positioning means nothing. Without the ultrasonic array, no mapping is possible. Without the accelerometer, depth calculation fails. Without compass heading, navigation breaks down. The sensors are the eyes, ears, and sense of balance for the entire system. Making them work reliably was the critical first step enabling all subsequent development.

