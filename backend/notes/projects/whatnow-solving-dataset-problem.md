# WhatNow: Solving the Dataset Problem Through Self-Training

The origin story of WhatNow is really about learning from failure. I had attempted two previous AI projects - Jam Hot for fruit recognition and Cirrus for weather prediction - and both failed because of fundamental problems with data quality. These failures taught me crucial lessons about what makes AI projects actually completable for individual developers.

Jam Hot was a computer vision project using the Fruit-360 dataset to recognize fruits from images. In validation, the model achieved 86% accuracy, which seemed promising. But when I tested it with real-world photos - fruits in my kitchen, images from my phone camera, real-world lighting conditions - the accuracy dropped to essentially 0%. The model had learned to recognize the specific studio conditions and backgrounds of the Fruit-360 dataset, not actual visual features of fruits. The dataset looked great on paper but was fundamentally inadequate for real-world application.

Cirrus was even more frustrating because I invested significantly more time before recognizing the insurmountable problem. I was building a wildfire prediction system using Canadian weather station data, processing 56 million weather records through sophisticated spatial interpolation algorithms. I built a dual-tier interpolation system achieving 100% grid coverage at over 21,000 records per second, implemented KD-tree spatial indexing, created genetic algorithms for model evolution - technically impressive work. But the underlying data had critical gaps: only 31% coverage for precipitation, completely missing wind speed and humidity, and 80% coverage for temperature at best. No amount of clever algorithms could compensate for data that simply didn't exist.

The pattern became clear: acquiring high-quality datasets is the single biggest barrier to completing AI projects as an individual developer. Large companies have data collection infrastructure, massive labeled datasets, and resources to clean and validate data. Individual developers have Kaggle datasets that don't match real-world conditions and public data with critical gaps. If I kept pursuing projects that depended on existing high-quality datasets, I would keep failing at the data acquisition stage.

WhatNow was born from asking: how do I build an AI project that doesn't have a data acquisition problem? The answer was to build a system that generates its own training data through actual usage. Every time I interact with WhatNow - inputting context, reviewing suggestions, selecting favorites, making final choices - I'm providing labeled training examples. The AI learns which activities I prefer given specific contexts, building its understanding from real usage rather than requiring a pre-existing dataset.

This isn't just pragmatic for completion - it's actually better for the use case. A pre-trained model would learn general population preferences but wouldn't know my specific tastes. A self-training system learns exactly my preferences through my actual behavior. The system is useful from day one (it makes reasonable initial suggestions) and gets progressively better as it learns more about me.

The insight that made WhatNow possible was recognizing that not all AI applications need massive datasets upfront. Reinforcement learning and online learning approaches can start with minimal data and improve through interaction. For personal utility applications, learning from one user's data over time can be more valuable than training on large but generic datasets.

This approach also made the project actually finishable. I could build a minimal viable version, deploy it, start using it, and watch it improve over time. There was no data acquisition phase where I might realize the dataset is inadequate after weeks of work. The "dataset" is my usage, which I control and can generate indefinitely.

The failure of Jam Hot and Cirrus was expensive in time and frustration, but it taught me something crucial about what kinds of AI projects are realistic for individual developers. Projects that depend on finding perfect existing datasets are high-risk. Projects that generate their own data through usage are much more likely to succeed. That lesson has guided my approach to the Folio RAG system too - it works with data I'm generating (atomic notes about my background) rather than depending on external datasets.

What I tell people about WhatNow is that it represents not just technical skills but strategic thinking about project selection. Building an AI system that can actually finish and deploy requires understanding the constraints you're working under and choosing architectures that work within those constraints. Self-training through usage eliminates the dataset acquisition problem, which turns out to be the difference between projects that fail in development and projects that make it to production.

