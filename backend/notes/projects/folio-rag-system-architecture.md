# Folio: RAG System Architecture and Implementation

The RAG (Retrieval Augmented Generation) system architecture in Folio demonstrates modern AI application design, combining semantic search, vector embeddings, and large language models into a cohesive system that answers questions accurately using a curated knowledge base.

The fundamental RAG workflow consists of three stages. First, when a user asks a question, the system generates an embedding vector for that question using OpenAI's text-embedding-3-small model. Second, the system searches the knowledge base for notes with similar embedding vectors using cosine similarity calculations in NumPy. Third, the top 5 most similar notes are retrieved and combined with the question into a prompt sent to GPT-4o-mini, which generates a natural language answer synthesizing information from the retrieved notes.

The vector similarity search is surprisingly simple yet powerful. Each note's embedding is a 1536-dimensional vector of floating point numbers. The query embedding is another 1536-dimensional vector. Cosine similarity calculates the angle between these vectors - semantically similar text produces vectors pointing in similar directions, giving high similarity scores (near 1.0). Dissimilar text produces vectors pointing in different directions, giving low similarity scores (near 0.0). This mathematical representation of semantic meaning enables searching by concept rather than keyword matching.

The retrieval strategy uses top-K selection where K equals 5 notes. Testing showed 5 notes provides enough context for comprehensive answers without overwhelming the LLM with irrelevant information. The notes are ranked by similarity score and the top 5 are selected. In practice, similarity scores for highly relevant notes tend to be 0.4-0.6, moderately relevant notes score 0.3-0.4, and weakly relevant notes score below 0.3. The top-5 cutoff naturally selects the most relevant information.

The prompt engineering for answer generation carefully balances guidance with flexibility. The system prompt instructs the LLM to answer as if it is me, speaking in first person, using provided context to give accurate personalized responses, and maintaining my authentic voice and personality. The user prompt provides the retrieved note content and the question. This structure gives the LLM enough guidance to stay on-task while allowing natural conversational responses.

The context combination strategy concatenates the top 5 notes with clear delimiters. Each note is prefixed with its title and the full content is included. This provides rich context for the LLM to synthesize information from multiple sources. When relevant notes come from different projects or topic areas, the LLM naturally weaves them together into coherent answers addressing all aspects of the question. This emergent synthesis behavior is one of LLM's strengths - combining information from disparate sources into unified responses.

The atomic notes design principle is critical to RAG effectiveness. Each note is self-contained, typically 200-500 tokens (roughly 150-375 words), covering a single coherent topic. Notes include enough context to make sense when retrieved individually - they don't assume you've read other notes first. This granularity enables precise retrieval where the system pulls exactly the relevant information without dragging along unrelated content from longer documents. It also enables flexible recombination where different queries pull different combinations of notes producing appropriate answers for varied questions.

The embedding model selection reflects cost and performance balance. OpenAI's text-embedding-3-small generates 1536-dimensional embeddings at $0.02 per million tokens. For Folio's 84 notes averaging 300 tokens each, total embedding cost is approximately $0.08 - essentially free. The model provides sufficient semantic understanding for the project's needs. Larger embedding models exist but cost more and provide diminishing returns for this use case. This pragmatic model selection demonstrates understanding actual requirements versus chasing maximum performance regardless of cost.

The local storage implementation uses a simple JSON file mapping note IDs to embeddings and metadata. Each entry stores the 1536-dimensional embedding vector as an array of floats, and metadata including note category, title, and content preview. The entire file for 84 notes is approximately 4MB - easily loaded into memory on any modern system. NumPy arrays provide efficient cosine similarity calculations completing searches in under 20 milliseconds. This performance is more than adequate for interactive chat and proves that sophisticated vector databases aren't necessary for datasets under 1000 notes.

The error handling and edge cases are carefully addressed. If embeddings storage is empty, the system returns helpful error messages rather than crashing. If a query produces no relevant results above a minimum similarity threshold, the system can acknowledge uncertainty rather than hallucinating answers. If the LLM generates responses that don't align with retrieved context, the system could implement verification checks. These defensive programming practices distinguish production systems from prototypes and ensure graceful degradation rather than catastrophic failures.

The scalability analysis projects performance at various dataset sizes. At 84 notes, query time is 20ms. At 200 notes, projected 25-30ms. At 500 notes, projected 40-50ms. At 1000 notes, projected 70-90ms. Beyond 1000 notes, architectural changes like approximate nearest neighbor search or vector databases become worthwhile. This data-driven scalability planning ensures the system doesn't prematurely optimize for scale that isn't needed while understanding when future optimization will be necessary.

The caching strategy is intentionally minimal. The embeddings are computed once and stored permanently. Query embeddings are computed fresh for each question since caching them provides minimal benefit - the embedding API call takes ~200ms but the response diversity makes caching hit rates low. This focused caching on the computationally expensive one-time operations (note embeddings) rather than trying to cache everything demonstrates understanding of where optimization matters.

The evaluation metrics track system performance quantitatively. Precision measures what fraction of retrieved notes are relevant - aiming for 4+/5 relevant notes for specific queries. Recall measures whether all relevant notes were retrieved - harder to measure but query variations help validate coverage. Answer quality is assessed subjectively but consistently by reviewing generated responses for accuracy, coherence, and voice authenticity. Query time is measured programmatically ensuring performance remains under 50ms threshold. These metrics enable data-driven system improvement.

The integration with LangChain provides production-ready orchestration. LangChain handles prompt template management, LLM API calls with retry logic, response parsing and validation, and conversation memory if implementing multi-turn dialogue. Using an established framework rather than raw API calls provides battle-tested reliability and reduces custom code maintenance. This pragmatic use of frameworks where they add value demonstrates mature engineering judgment.

From a technical portfolio perspective, the RAG architecture demonstrates understanding of modern AI application patterns, semantic search and vector embeddings, LLM prompting and response generation, system design balancing cost and performance, and production-ready error handling and monitoring. It proves I can build AI systems beyond just calling APIs - I understand the underlying architecture and can make informed design decisions.

What I learned from building Folio's RAG system is that effective AI applications are more about architecture and integration than about bleeding-edge AI techniques. The embedding model is off-the-shelf. The similarity search is basic linear algebra. The LLM is OpenAI's smallest model. But the combination works excellently because each piece is used appropriately. This lesson about leveraging existing tools effectively rather than building everything custom applies broadly to software engineering.

The RAG system architecture in Folio represents modern best practices for AI-augmented applications: semantic search for intelligent retrieval, LLMs for natural language generation, curated knowledge bases for accuracy, and pragmatic technology choices balancing cost, performance, and maintainability. This architecture pattern will increasingly be standard for AI applications, making experience building systems like Folio directly valuable for AI/ML engineering roles.

