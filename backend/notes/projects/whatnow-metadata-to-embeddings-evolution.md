# WhatNow: Evolution from Manual Metadata to AI Embeddings

One of the significant technical pivots in WhatNow was moving from manual metadata to AI-generated embeddings for representing activities. This change fundamentally improved how the system understands and compares activities, while also making the codebase lighter and more maintainable.

The original approach used manual metadata - I hand-coded 15+ fields for each of the 1,249 activities. Fields like energy_level (1-10), social_factor (1-10), time_required (minutes), indoor_outdoor (boolean), physical_mental (spectrum), creative_analytical (spectrum), and various category tags. For each activity, I had to subjectively judge where it fell on all these dimensions. This took enormous time, and my judgments were inevitably inconsistent and incomplete.

The manual metadata approach had several problems. First, it was incredibly time-consuming - rating 1,249 activities across 15+ dimensions meant making over 18,000 individual judgments. Second, it was subjective - is "learning to juggle" a 6 or 7 on the energy scale? Is it 60% physical and 40% mental, or 70/30? Different days I'd make different judgments. Third, it was limited - no matter how many dimensions I added, I couldn't capture all the nuanced similarities and differences between activities. "Reading poetry" and "journaling about emotions" are similar in ways my manual tags couldn't express.

The solution was semantic embeddings. Instead of hand-coding features, I use a sentence transformer model to convert each activity's description into a 384-dimensional vector that captures semantic meaning. Similar activities naturally end up with similar vectors because they share semantic content. The model understands that "baking bread" and "making pasta from scratch" are related, even though I never explicitly tagged them as similar cooking activities.

The embeddings are generated using sentence transformers (specifically 'all-MiniLM-L6-v2'), which takes the activity name and description and produces a dense vector representation. These vectors capture semantic relationships automatically - activities about creativity cluster together, physical activities cluster together, social activities cluster together, without me having to manually categorize them.

For the contextual bandit system, the embeddings become features that the AI can learn to weight. Instead of learning "increase the weight for high-energy activities when energy context is high," it learns "increase weights for activities with similar embedding dimensions to previously selected activities when in similar contexts." This is more flexible and expressive than my manual metadata could ever be.

The technical transition required rethinking the whole activity representation system. I wrote scripts to generate embeddings for all activities, modified the database schema to store embedding vectors, updated the contextual bandit feature engineering to work with high-dimensional embeddings instead of hand-coded features, and verified that the system still worked correctly with the new representation. The switch was significant but necessary.

An unexpected benefit was deployment simplicity. The original system with manual metadata logic was becoming complex, with lots of conditional logic about how different metadata fields should combine. The embedding-based system is conceptually simpler - activities are points in embedding space, and the AI learns which regions of that space correspond to my preferences in different contexts. This simpler conceptual model made the code cleaner and easier to debug.

Performance considerations mattered too. Working with 384-dimensional embeddings seems like it should be computationally expensive, but modern numpy and JavaScript typed arrays handle it efficiently. The bottleneck in recommendation generation isn't the embedding math, it's database queries and API latency. The embedding computations themselves are fast enough to not impact user experience.

The quality of recommendations improved noticeably after the switch. With manual metadata, I would sometimes get suggestions that were technically correct according to my tags but felt wrong in context. With embeddings, the semantic understanding means suggestions feel more naturally related to what I'm looking for. The system "understands" activities at a deeper level than my hand-coded features could capture.

This evolution from manual to AI-generated representations is a pattern I've seen in my own development and in the field generally. Early machine learning relied heavily on manual feature engineering - experts coding their knowledge into features. Modern approaches increasingly use learned representations - letting models discover features from data. My journey with WhatNow parallels that broader evolution, moving from hand-coded features to AI-discovered semantic structure.

What I learned from this pivot is that sometimes the right solution isn't optimizing your current approach - it's recognizing when a fundamentally different approach is better. I could have spent more time refining my manual metadata, adding more dimensions, tweaking my rating schemes. Instead, I recognized that semantic embeddings solve the core problem more elegantly. Being willing to make that kind of fundamental architectural change is crucial for building systems that truly work well.

For employers, this demonstrates several things: I understand modern ML techniques like embeddings and semantic similarity, I can make significant architectural changes when needed rather than being wedded to original designs, I think about tradeoffs between manual engineering and learned representations, and I can take projects through major technical transitions while keeping them functional. The fact that WhatNow works better after this change validates the decision to make the pivot rather than optimizing the wrong approach.

