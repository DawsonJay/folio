# Folio: Test-Driven Development and Iterative Validation

The test-driven development approach in Folio demonstrates mature engineering methodology, validating the RAG system incrementally with quantitative metrics before scaling. This disciplined iteration strategy distinguished between hypothetical design and proven implementation.

The initial hypothesis was that atomic notes with semantic search would enable accurate question-answering about my portfolio. Rather than assuming this would work and building 200 notes before testing, I designed a validation strategy: build 20 notes as a minimal test set, run structured queries, measure retrieval precision, assess answer quality, and use results to inform whether to scale or pivot. This test-first mentality proves the approach works before investing significant effort.

Iteration 1 created 20 notes: 10 general background notes covering skills, work experience, values, and education, plus 10 WhatNow project deep-dive notes covering various aspects of that project in detail. This 2:1 ratio of general to project-specific notes tested whether the system could distinguish between broad background queries and project-focused queries. The deliberate imbalance made a good test case - would WhatNow notes dominate all results or would the system appropriately select based on query relevance?

The test query design was systematic and comprehensive. Six WhatNow-focused queries like "Tell me about WhatNow" and "How does the two-layer learning work?" should retrieve mostly WhatNow notes. Six general queries like "What's your React experience?" and "Why do you want to work in Canada?" should retrieve mostly general background notes. Five edge case queries like "Tell me about a project using contextual bandits" tested queries where multiple note types could be relevant. These 17 queries provided quantitative metrics for system performance.

The results from Iteration 1 exceeded expectations. WhatNow-focused queries retrieved an average of 4.67 out of 5 WhatNow notes, demonstrating excellent precision for project-specific queries. General queries retrieved only 0.50 out of 5 project-specific notes on average, proving the system maintained good boundaries and didn't over-represent projects when answering general questions. Answer quality was subjectively assessed as good - responses were coherent, relevant, and maintained first-person voice. Query time stayed under 20 milliseconds for the local search, well below the 50ms threshold.

Iteration 2 expanded to 31 notes by adding 10 moh-ami project deep-dive notes. This tested multi-project behavior - with two projects deeply documented, would the system confuse them? Would a query about one project retrieve notes from the other? The critical test query "Tell me about moh-ami" should retrieve only moh-ami notes with zero WhatNow contamination if the semantic search truly understands project boundaries.

The Iteration 2 results validated multi-project separation. WhatNow precision stayed stable at 4.50 out of 5 (slight expected decrease with more total notes to choose from). The moh-ami query "Tell me about moh-ami" retrieved 5 out of 5 moh-ami notes with 0 WhatNow notes, proving perfect project isolation through semantic search alone without any metadata filtering. General query separation remained stable at 0.50 out of 5 project notes. Query time remained under 20 milliseconds despite 50% more content. This stability under growth validated the scalability of the approach.

The cross-project query testing in Iteration 2 explored whether the system could appropriately mix projects when relevant. The query "How do you approach problem-solving?" should pull examples from multiple projects since I solve problems in all of them. Results showed 1 WhatNow note and 2 moh-ami notes in the top 5, appropriately synthesizing examples from both projects. This demonstrated the system intelligently combines information across project boundaries when the query warrants it.

The metrics tracking provided data-driven decision making. Each iteration measured WhatNow precision (fraction of WhatNow queries retrieving mostly WhatNow notes), general separation (fraction of general queries avoiding project notes), query time (milliseconds for semantic search), answer quality (subjective coherence and accuracy), and storage size (MB for embeddings file). These metrics revealed whether performance degraded, stayed stable, or improved with scale, informing architectural decisions.

The scalability projections used iteration data to forecast future performance. From 21 notes to 31 notes, query time stayed constant, suggesting linear storage growth with constant query time up to at least 100 notes. Precision stayed stable or improved, suggesting semantic search quality doesn't degrade with more notes. These observations project that the system comfortably scales to 100+ notes without architectural changes, and possibly to 500+ notes before optimization becomes necessary. This data-driven scalability planning prevents premature optimization while understanding future needs.

The test automation made iterations fast and repeatable. The test_retrieval.py script runs 17 predefined queries, records which notes are retrieved for each, generates answers using the LLM, counts project-specific notes in results, calculates averages and statistics, and produces formatted output for analysis. Running this script takes about 2 minutes and provides comprehensive performance assessment. This automation eliminates manual testing tedium and ensures consistent metrics across iterations.

The answer quality assessment, while subjective, followed consistent criteria. Accuracy means the answer correctly reflects information in my background and projects. Relevance means the answer addresses the question asked rather than wandering off topic. Coherence means the answer flows naturally and doesn't contradict itself. Voice authenticity means it sounds like me speaking, not a generic description. Completeness means important relevant details are included. These criteria enable subjective but systematic quality evaluation.

The failure analysis examined queries that didn't perform optimally. "Show me the WhatNow demo" retrieved the correct note with the URL but the LLM generated a placeholder link rather than using the actual URL from the note. This identified a prompt engineering issue rather than retrieval failure. "How did you solve the dataset problem?" retrieved 3/5 WhatNow notes versus 4-5 expected, suggesting this query's phrasing could be more specific or the relevant notes could be better written. These failure analyses drive continuous improvement.

The test-driven benefits throughout Folio development were substantial. Catching the link placeholder issue early meant fixing the prompt before generating hundreds of notes. Validating multi-project separation at 31 notes provided confidence to scale to 84 notes without testing each addition. Quantitative metrics enabled explaining system behavior to others rather than vague claims. The systematic approach converted an untested hypothesis into a proven system backed by data.

From a professional methodology perspective, the test-driven approach demonstrates engineering discipline that employers value. Building minimal test sets before scaling, defining clear success metrics, automating testing for repeatability, documenting results comprehensively, making data-driven decisions about architecture, and iterating based on measured performance - these practices distinguish professional engineering from hobbyist coding. They ensure projects don't accumulate technical debt or pursue flawed approaches at scale.

What I learned from test-driven RAG development is that AI systems benefit enormously from quantitative evaluation even when building for yourself. It's tempting to skip testing and just build all the notes, assuming it will work. Testing revealed specific issues (link generation) and validated assumptions (multi-project separation) that speculation alone couldn't confirm. This lesson about systematic validation applies beyond AI to any complex system where behavior emerges from component interactions.

The test-driven development approach in Folio - minimal test sets before scaling, comprehensive test queries covering edge cases, quantitative metrics for objective assessment, iterative validation and refinement, automation for repeatability, and data-driven architectural decisions - represents mature engineering practices that produce reliable systems rather than hoping things work. This methodology proves I can build production-ready AI applications with proper testing discipline, exactly what employers need in AI/ML roles.

